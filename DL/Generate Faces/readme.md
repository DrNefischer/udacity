# Project: Generate Faces

## Review:

A few resources to further boost your knowledge in this area:

* To gain more intuition about GANs in general, I would suggest you take a look at [this blog post](https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7).
* If you want to gain intuition about convolution and transpose convolution arithmetic, I would suggest referring to [this paper](https://arxiv.org/abs/1603.07285).
* For more advanced techniques on training GANs, you can refer to [this paper](https://arxiv.org/abs/1606.03498).
* You might have seen that reduction in loss and the quality of images do not track closely which is unlike other networks that we have seen earlier. To create a GAN that has this relationship that lower loss will result in better images, take a look at [Wasserstein GAN](https://arxiv.org/abs/1701.07875). This GAN is based on a different loss function which is known as Wasserstein Loss.
* GAN is a prominent area of research. You can keep track of the new techniques introduced in this category [here](https://paperswithcode.com/methods/category/generative-adversarial-networks).
* You are currently using even-sized filters. Instead of using even-sized filters, please use odd-sized filters as the concept of a central or anchor pixel is more defined in the odd-sized filters. In odd-sized kernels, we have the same number of pixels on all sides from the anchor pixel. When using an even-sized filter, since this is not the case, it leads to aliasing errors. You can refer to page 2 of [this document](https://www.cs.tau.ac.il/~dcor/Graphics/adv-slides/sampling05.pdf) for different kinds of aliasing errors. I would recommend using a kernel size of 5 for your network here.
* Please use a stride of 1 in the last layer to avoid checkerboard-like artifacts in the generated images. You can read more about the phenomenon in [this blog post](https://distill.pub/2016/deconv-checkerboard/).
* You could have implemented [one-sided label smoothing](https://arxiv.org/abs/1606.03498) for the real loss.
* Also, you can try using squared loss too. It has been found to work better than the cross-entropy loss. You can take a look at [this paper](https://arxiv.org/abs/1611.04076) where the least-squares loss was introduced. The images generated by Least Squares GAN were found to be of better quality than the classical GANs.
